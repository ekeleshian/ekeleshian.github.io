<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Data Science N00b]]></title><description><![CDATA[Digital journal describing my career U-turn towards Data Science.]]></description><link>https://ekeleshian.github.io/</link><generator>RSS for Node</generator><lastBuildDate>Sun, 24 Jun 2018 03:42:18 GMT</lastBuildDate><item><title><![CDATA[My last day at school]]></title><description><![CDATA[My last day working at a middle school stirred up a mixed bag of emotions for me.  It was one of those moments in my life that felt heavy…]]></description><link>https://ekeleshian.github.io//last_day/</link><guid isPermaLink="false">https://ekeleshian.github.io//last_day/</guid><pubDate>Sat, 23 Jun 2018 15:40:24 GMT</pubDate><content:encoded>&lt;p&gt;My last day working at a middle school stirred up a mixed bag of emotions for me.  It was one of those moments in my life that felt heavy and familiar — it resembled the emotions that I experienced when I graduated college or the time I moved to San Francisco by myself. There were feelings of sadness, no doubt, but I’m not sure I will ever find a perfect combination of words to describe yesterday’s emotions.  &lt;/p&gt;
&lt;p&gt;Before the last day of school, I pictured what it would look like: an auditorium filled with a ton of techie parents, wearing their usual jeans-tshirt-blazer-for-special-occasions ensemble, happily watching/documenting their child’s middle school graduation, and then listening to disappointingly plain speeches about life achievements from adults who don’t educate. And following the ceremony and teary-eyed student goodbyes, would follow the luncheon, in which our final goodbyes to the faculty would take place, whether it be a temporary ‘see you later’ or permanent departure. Of the farewell toasts that would be given, I pictured mine to be mediocre, as I didn’t really develop social relationships with my colleagues.   Picturing the dreadful closure with the school really didn’t sound appealing, and I actually tried to think of ways to avoid going.  I was worried that the very nature of the last day’s program was going to lead to an improper, unprofessional mark of closure with me and the school.&lt;/p&gt;
&lt;p&gt;Despite my pessimism, I sucked it up and went.  The ceremony’s speeches were plain as expected.  Very disappointing, as that room was filled with over a billion dollars worth of Silicon Valley talent. The kids looked lovely, though.  They were so excited, so scared, and so sad all at the same time, as they approached me for our final goodbyes.  That moment, of seeing all the kids take their last minute pictures with each other, making their final farewells to the staff, reuniting with their families with hugs and kisses, that moment was truly beautiful.  I felt so present, so happy to be there, seeing the kids together one last time and bidding our farewells..    &lt;/p&gt;
&lt;p&gt;It was at that point that I should’ve left. But recalling a conversation with my boss, one of the few obligations requested was to attend the luncheon.  So, I felt the need to follow my word.  The luncheon was just the thing that I worried would happen - spoil my closure with my the school.  The farewell toast that was given to me was terse, unprepared, and partially didn’t make sense.  And that was it.  This was the school’s way to properly acknowledge my achievements and bid me farewell.  Hearing the other farewell toasts after mine, which were all beautifully delivered with prepared scripts, made me feel even worse about it. It indeed reminded me that I had no friends at work. It reminded me of the lack of support I get on a normal basis and the immense amount of work that goes unrecognized, unacknowledged. I felt alone.  I felt empty. I felt my contributions were void. It didn’t matter. I was disposable.&lt;/p&gt;
&lt;p&gt;Closure at work, especially when 100% of times are stormy, dysfunctional, leaderless, and multi-faceted, is super important. A meaningful closure is part of the backbone of professionalism. It serves as a final channel of communication between the departing employee and the company, in which the employee and employer can exchange their final feedback, words, and sentiments. I feel beyond grateful that I received recognition from the children and their parents, as that’s what truly matters.  The various forms the children expressed their gratitude towards me as a math teacher made the farewell process so meaningful, that I fantasize that this was the way my career as a teacher ended.  &lt;/p&gt;
&lt;p&gt;Time to move onward.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Part 1  Cleaning Up Data]]></title><description><![CDATA[In this post, I will discuss my beginning approach to my first project, which revolves around financial literacy in the United States…]]></description><link>https://ekeleshian.github.io//cleaning-up-data/</link><guid isPermaLink="false">https://ekeleshian.github.io//cleaning-up-data/</guid><pubDate>Wed, 13 Jun 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;In this post, I will discuss my beginning approach to my first project, which revolves around financial literacy in the United States.  &lt;/p&gt;
&lt;p&gt;With an extensive Google Scholar search, I found relevant data about the topic.  The database had some messy qualities.  I recall opening
the csv file into Excel and feeling totally dumbfounded; the data
looked meaningless — just rows of arbitrary alphanumeric
sequences that deemed it impossible to do any analysis.
Fortunately, in the directory of this database also included a pdf file of a key of all
alpha numeric codes and their translations.  It contained the
question and multiple choices of every survey question from the data.
With this access, I realized that luck played a huge role with
getting this project started (and that Google is amazing!). So, this prompted
me to clean up the data, reformatting the data so that it was actually
readable.   &lt;/p&gt;
&lt;p&gt;I wrote a python file, called “schema.py”, that contained dictionaries of the survey’s
alphanumeric codes and their respective translations. I first started decoding the attributes that expressed each survey question; for instance, the code “STATEQ” decodes to State.  Since there were 126 survey questions, there were 126 attributes, each decoded according to the nature of the survey question.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;survey_attributes &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;survey_code&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;attribute&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Then, I decoded the various codes for survey output.  For instance, if an item in the third question had a ‘2’, and if the said question happened to be a true/false question, then the value ‘2’ would be decoded to “False”. The decoding process is an integral part in cleaning up the data.  The more time you spend cleaning up the data, the much higher chances of generating successful and insightful results about the data.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;output_codes &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;output1&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;True&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;I then wrote a new python file, named “main.py” which contained a pipeline function, that when called, causes a chain of events in which the data frame tranforms into a readable and organized platform to work with.  &lt;/p&gt;
&lt;p&gt;In my main function, I write the path of the csv file, save it as a pandas dataframe object, and finally save and run the dataframe through the aforementioned pipeline. &lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; pd
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; schema

&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;new_columns&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;df&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; 
	df&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;rename&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;columns &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; schema&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;survey_attributes&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; inplace &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; df

&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;convert_row_values&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;df&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; k &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; schema&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;output_codes&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
		dictionary &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;k&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;output_codes&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;k&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
		df&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;replace&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;to_replace &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; dictionary&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; inplace&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; df


&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;dataframe&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
	modified_columns_df &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; new_columns&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;dataframe&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	master_df &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; convert_row_values&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;modified_columns_df&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; master_df

&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
	csv_path &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;/Users/elizabethkeleshian/financial_literacy_analytics/2015_state_data.csv&apos;&lt;/span&gt;
	df &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;read_csv&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;csv_path&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	results &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pipeline&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;df&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;You will notice that when calling the pipeline function, two other functions are called, in order to convert the column and row data into workable content.  &lt;/p&gt;
&lt;p&gt;And walla! You get a workable dataframe.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;usr&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;bin&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;python2&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;7&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;home&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;elizabeth&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;financial_literacy&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;main_&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;py
       RespondentID                 State      CensusDivision CensusRegion  \
&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010001&lt;/span&gt;               Arizona            Mountain         West   
&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010002&lt;/span&gt;                  Ohio  East North Central      Midwest   
&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010003&lt;/span&gt;              New York     Middle Atlantic    Northeast   
&lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010004&lt;/span&gt;               Florida      South Atlantic        South   
&lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010005&lt;/span&gt;            New Jersey     Middle Atlantic    Northeast   
&lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010006&lt;/span&gt;              Missouri  West North Central      Midwest   
&lt;span class=&quot;token number&quot;&gt;6&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010007&lt;/span&gt;               Florida      South Atlantic        South   
&lt;span class=&quot;token number&quot;&gt;7&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010008&lt;/span&gt;               Arizona            Mountain         West   
&lt;span class=&quot;token number&quot;&gt;8&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010009&lt;/span&gt;        South Carolina      South Atlantic        South   
&lt;span class=&quot;token number&quot;&gt;9&lt;/span&gt;        &lt;span class=&quot;token number&quot;&gt;2015010010&lt;/span&gt;              New York     Middle Atlantic    Northeast   
&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010011&lt;/span&gt;            Washington             Pacific         West   
&lt;span class=&quot;token number&quot;&gt;11&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010012&lt;/span&gt;               Florida      South Atlantic        South   
&lt;span class=&quot;token number&quot;&gt;12&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010013&lt;/span&gt;           Connecticut         New England    Northeast   
&lt;span class=&quot;token number&quot;&gt;13&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010014&lt;/span&gt;                 Texas  West South Central        South   
&lt;span class=&quot;token number&quot;&gt;14&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010015&lt;/span&gt;                Kansas  West North Central      Midwest   
&lt;span class=&quot;token number&quot;&gt;15&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010016&lt;/span&gt;            California             Pacific         West   
&lt;span class=&quot;token number&quot;&gt;16&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010017&lt;/span&gt;           Connecticut         New England    Northeast   
&lt;span class=&quot;token number&quot;&gt;17&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010018&lt;/span&gt;            California             Pacific         West   
&lt;span class=&quot;token number&quot;&gt;18&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010019&lt;/span&gt;                 Texas  West South Central        South   
&lt;span class=&quot;token number&quot;&gt;19&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010020&lt;/span&gt;          Pennsylvania     Middle Atlantic    Northeast   
&lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010021&lt;/span&gt;                 Texas  West South Central        South   
&lt;span class=&quot;token number&quot;&gt;21&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010022&lt;/span&gt;                 Texas  West South Central        South   
&lt;span class=&quot;token number&quot;&gt;22&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010023&lt;/span&gt;            California             Pacific         West   
&lt;span class=&quot;token number&quot;&gt;23&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010024&lt;/span&gt;          Pennsylvania     Middle Atlantic    Northeast   
&lt;span class=&quot;token number&quot;&gt;24&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010025&lt;/span&gt;              Michigan  East North Central      Midwest   
&lt;span class=&quot;token number&quot;&gt;25&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010026&lt;/span&gt;          Pennsylvania     Middle Atlantic    Northeast   
&lt;span class=&quot;token number&quot;&gt;26&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010027&lt;/span&gt;                 Texas  West South Central        South   
&lt;span class=&quot;token number&quot;&gt;27&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010028&lt;/span&gt;               Florida      South Atlantic        South   
&lt;span class=&quot;token number&quot;&gt;28&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010029&lt;/span&gt;                  Ohio  East North Central      Midwest   
&lt;span class=&quot;token number&quot;&gt;29&lt;/span&gt;       &lt;span class=&quot;token number&quot;&gt;2015010030&lt;/span&gt;           Connecticut         New England    Northeast&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;From here, I then imported the matplotlib.pyplot subpackage to generate some fun data visualizations.  Stay tuned to learn how I went about this as a data science n00b…&lt;/p&gt;</content:encoded></item><item><title><![CDATA[From Math Teacher to Data Science n00b]]></title><description><![CDATA[I entered my late twenties questioning everything…  Moving to SF in 2016 caused some major turn arounds in my life. 
Various factors led…]]></description><link>https://ekeleshian.github.io//new-beginnings/</link><guid isPermaLink="false">https://ekeleshian.github.io//new-beginnings/</guid><pubDate>Sun, 10 Jun 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I entered my late twenties questioning everything… &lt;/p&gt;
&lt;p&gt;Moving to SF in 2016 caused some major turn arounds in my life.
Various factors led from one thing to the next, causing a sequence of events that
were so atypical compared to my prior way of life.  I can almost describe
my chain of events as spontaneous in a way; life has felt fast and exciting.
But with this spontaneity nature comes also a determined
side. Paradoxical, indeed, but the truth will always
hold that I have a deep affinity for math, especially statistics.
My dream has always been to find work where my math knowledge
puts me in an advantage and generates results that are impactful.&lt;br&gt;
Teaching middle school math somehow didn’t cut it.&lt;/p&gt;
&lt;p&gt;2016-2017 indeed has been an era of epiphanies.  Many realizations
were made that became game-changers in my life - one
being that I did not want to continue teaching.  Something very deep
in my instincts, a feeling that is too internal to describe in words,
could explain why teaching is ultimately not for me. This blog is not intended
to pour out those feelings.  This blog is just a representation of my
journal describing my progress as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Newbie&quot;&gt;n00b&lt;/a&gt;
in data science.  &lt;/p&gt;</content:encoded></item><item><title><![CDATA[From Math Teacher to Data Science n00b]]></title><description><![CDATA[I entered my late twenties questioning everything…  Moving to SF in 2016 caused some major turn arounds in my life. 
Various factors led…]]></description><link>https://ekeleshian.github.io//recent-project/</link><guid isPermaLink="false">https://ekeleshian.github.io//recent-project/</guid><pubDate>Sun, 10 Jun 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I entered my late twenties questioning everything… &lt;/p&gt;
&lt;p&gt;Moving to SF in 2016 caused some major turn arounds in my life.
Various factors led from one thing to the next, causing a sequence of events that
were so atypical compared to my prior way of life.  I can almost describe
my chain of events as spontaneous in a way; life has felt fast and exciting.
But with this spontaneity nature comes also a determined
side. Paradoxical, indeed, but the truth will always
hold that I have a deep affinity for math, especially statistics.
My dream has always been to find work where my math knowledge
puts me in an advantage and generates results that are impactful.&lt;br&gt;
Teaching middle school math somehow didn’t cut it.&lt;/p&gt;
&lt;p&gt;2016-2017 indeed has been an era of epiphanies.  Many realizations
were made that became game-changers in my life - one
being that I did not want to continue teaching.  Something very deep
in my instincts, a feeling that is too internal to describe in words,
could explain why teaching is ultimately not for me. This blog is not intended
to pour out those feelings.  This blog is just a representation of my
journal describing my progress as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Newbie&quot;&gt;n00b&lt;/a&gt;
in data science.  &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Exploring Data on Financial Literacy]]></title><description><![CDATA[My recent project, which I feel very excited about,
is centered on financial literacy data in the US.  Due to my academic 
background in…]]></description><link>https://ekeleshian.github.io//rule-of-thumb/</link><guid isPermaLink="false">https://ekeleshian.github.io//rule-of-thumb/</guid><pubDate>Sun, 10 Jun 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;My recent project, which I feel very excited about,
is centered on financial literacy data in the US.  Due to my academic
background in this topic, I have been very eager to explore financial
literacy amongst Americans. Luckily, I found some awesome
data with a pretty extensive Google Scholar search. A questionnairre was
distributed in 2015, asking Americans about their monetary management skills,
spending behavior, investments (if any), plans for
retirement, and a six-question quiz assessing the surveyee’s
financial knowledge.  The database contained more than 27,000 survey submissions, each
generating more than 100 data points of responses.&lt;/p&gt;
&lt;p&gt;This database had some messy qualities.  I recall opening
the csv file into Excel and feeling totally dumbfounded; the data
looked meaningless — just rows of arbitrary alphanumeric
sequences that deemed it impossible to do any analysis.
Fortunately, in the directory of this database also included a pdf file of a key of all
alpha numeric codes and their translations.  It contained the
question and multiple choices of every survey question from the data.
With this access, I realized that luck played a huge role with
getting this project started (and that Google is amazing!). So, this prompted
me to clean up the data, reformatting the data so that it was actually
readable.   &lt;/p&gt;
&lt;p&gt;I wrote a python file that contained dictionaries of the survey’s
alphanumeric codes and their respective translations.&lt;/p&gt;
&lt;p&gt;![Decoding Questions](./Screen Shot 2018-06-10 at 4.14.43 PM.png)&lt;/p&gt;
&lt;p&gt;I wrote a chain of functions that call &lt;/p&gt;</content:encoded></item></channel></rss>