{"version":3,"sources":["webpack:///path---reading-from-text-files-dbac1311eb5044016498.js","webpack:///./.cache/json/reading-from-text-files.json"],"names":["webpackJsonp","520","module","exports","data","site","siteMetadata","title","author","markdownRemark","id","html","frontmatter","date","pathContext","slug","previous","fields","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,cAAAC,OAAA,wBAAsDC,gBAAmBC,GAAA,6HAAAC,KAAA,8SAAAC,aAAscL,MAAA,iCAAAM,KAAA,uBAAsEC,aAAgBC,KAAA,4BAAAC,UAA+CC,QAAUF,KAAA,sBAA4BH,aAAgBL,MAAA,+BAAsCW,KAAA","file":"path---reading-from-text-files-dbac1311eb5044016498.js","sourcesContent":["webpackJsonp([93995046729774],{\n\n/***/ 520:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"math lizard\",\"author\":\"Elizabeth Keleshian\"}},\"markdownRemark\":{\"id\":\"/home/mathlizard/myblog/ekeleshian.github.io/src/pages/reading_from_text_files/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<h2>The Background</h2>\\n<p>I’m writing Google Cloud Functions to set up a . Google’s server has allocated 512 MB of memory for each function.  A few of my functions need to handle a large csv file, more than the allocated amount.  For I stored the csv file in a bucket from Google Cloud Storage</p>\",\"frontmatter\":{\"title\":\"Reading from blobs efficiently\",\"date\":\"December 20, 2018\"}}},\"pathContext\":{\"slug\":\"/reading_from_text_files/\",\"previous\":{\"fields\":{\"slug\":\"/prepping-for-dsr/\"},\"frontmatter\":{\"title\":\"Prepping for a New Chapter\"}},\"next\":null}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---reading-from-text-files-dbac1311eb5044016498.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"math lizard\",\"author\":\"Elizabeth Keleshian\"}},\"markdownRemark\":{\"id\":\"/home/mathlizard/myblog/ekeleshian.github.io/src/pages/reading_from_text_files/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<h2>The Background</h2>\\n<p>I’m writing Google Cloud Functions to set up a . Google’s server has allocated 512 MB of memory for each function.  A few of my functions need to handle a large csv file, more than the allocated amount.  For I stored the csv file in a bucket from Google Cloud Storage</p>\",\"frontmatter\":{\"title\":\"Reading from blobs efficiently\",\"date\":\"December 20, 2018\"}}},\"pathContext\":{\"slug\":\"/reading_from_text_files/\",\"previous\":{\"fields\":{\"slug\":\"/prepping-for-dsr/\"},\"frontmatter\":{\"title\":\"Prepping for a New Chapter\"}},\"next\":null}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/reading-from-text-files.json\n// module id = 520\n// module chunks = 93995046729774"],"sourceRoot":""}